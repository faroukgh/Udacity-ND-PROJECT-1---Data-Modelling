# Introduction
A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

# Project Description
To define fact and dimension tables for a star schema for a particular analytic focus, and write an ETL pipeline that transfers data from files in two local directories into these tables in Postgres using Python and SQL.

# Project scope
A. The purpose of Sparkify database  and its analytical goals
Sparkify data in scattered logs and meta data will not provide this easy and rapid quries. A simple and better way to represent data in a structured way is to model the data to allow for easy read and analysis is to used a relational database (postgreSQL) since the company is a startup and has less or no big data. This database will also allow for queries on the data and very fast.

B. Justification for  database schema design and ETL pipeline
The many focus of the analytics at Sparkify is to analyse what songs users are listening to. A star schema design model will provide this need for the analytics team. It allows for rapid queries on tables.

C. Examples of queries and results for song play analysis
1. %sql SELECT * FROM songplays LIMIT 5;
 * postgresql://student:***@127.0.0.1/sparkifydb
5 rows affected.

2. %sql SELECT * FROM songplays LIMIT 5;
 * postgresql://student:***@127.0.0.1/sparkifydb
5 rows affected.

3.%sql SELECT * FROM songs LIMIT 5;
 * postgresql://student:***@127.0.0.1/sparkifydb
5 rows affected.

4.%sql SELECT * FROM artists LIMIT 5;
 * postgresql://student:***@127.0.0.1/sparkifydb
5 rows affected.

5. %sql SELECT * FROM time LIMIT 5;
 * postgresql://student:***@127.0.0.1/sparkifydb
5 rows affected.

# N/B screenshot of data displayed are in the asset folder

# Song Dataset
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. 

# Log Dataset
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

# Tools
- Python
- Jupyter Notebook
- PostgreSQL

# Setup and Running project
- Install and configure Python, Jupyter and PostgreSQL
- Connect Postgresql to 'create_tables.py' and 'etl.py' files
- Run 'create_tables.py' file and run 'etl.py' file
    Using !python create_tables.py and !python etl.py in the console
- Run queries on 'sql_queries.py'
- Run test.ipynb to get sample queries result for analysises

# Reference 
https://www.pitt.edu/~naraehan/python3/file_path_cwd.html
https://knowledge.udacity.com/questions/434396
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html